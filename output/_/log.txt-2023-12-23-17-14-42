** Config **
adapter: False
adapter_dim: None
adaptformer: True
backbone: CLIP-ViT-B/16
batch_size: 32
bias_tuning: False
bn_tuning: False
classifier: CosineClassifier
ctx_init: a photo of a
data_path_train: /home/wzz/COCO/train2017
data_path_val: /home/wzz/COCO/val2017
dataset: 
deterministic: True
expand: 24
full_tuning: False
gpu: 0
imb_factor: None
init_head: text_feat
ln_tuning: False
lora: False
loss_type: Focal
lr: 0.0001
micro_batch_size: 32
model_dir: None
momentum: 0.9
n_ctx: 4
num_epochs: 120
num_workers: 8
output_dir: ./output/_
partial: None
prec: amp
print_freq: 10
resolution: 224
root: /home/wzz/COCO
scale: 25
seed: 33
ssf_attn: False
ssf_ln: False
ssf_mlp: False
test_ensemble: True
test_only: False
test_path: annotations/instances_val2017.json
test_train: False
train_path: /home/wzz/SCPNet/dataset/coco_lt_train.txt
vpt_deep: True
vpt_len: None
vpt_shallow: False
weight_decay: 0.0001
zero_shot: False
************
Setting fixed seed: 33
mean: [0.48145466, 0.4578275, 0.40821073]
std: [0.26862954, 0.26130258, 0.27577711]
loading annotations into memory...
Done (t=1.18s)
creating index...
index created!
Building model
Loading CLIP (backbone: CLIP-ViT-B/16)
Visual prompt length set to 10
Adapter bottle dimension set to 2
Turning off gradients in the model
Turning on gradients in the prompt
Turning on gradients in the tuner
Turning on gradients in the head
Total params: 149840933
Tuned params: 156708
Head params: 61440
Initialize head with text features
Initialize tensorboard (log_dir=./output/_/tensorboard)
epoch [1/120] batch [10/60] time 0.286 (0.743) data 0.001 (0.243) loss 16.9253 (17.3657) lr 1.0000e-04 eta 1:28:59
epoch [1/120] batch [20/60] time 0.243 (0.514) data 0.000 (0.125) loss 14.1682 (15.9730) lr 1.0000e-04 eta 1:01:30
epoch [1/120] batch [30/60] time 0.374 (0.431) data 0.000 (0.084) loss 10.3876 (13.3354) lr 1.0000e-04 eta 0:51:30
epoch [1/120] batch [40/60] time 0.268 (0.391) data 0.021 (0.064) loss 4.1244 (8.9673) lr 1.0000e-04 eta 0:46:36
epoch [1/120] batch [50/60] time 0.220 (0.376) data 0.001 (0.051) loss 0.1172 (3.5466) lr 1.0000e-04 eta 0:44:49
epoch [1/120] batch [60/60] time 0.139 (0.355) data 0.001 (0.043) loss 0.3620 (1.3919) lr 1.0000e-04 eta 0:42:14
Evaluate on the test set
mAP head: 0.5056587966542341, mAP medium: 0.6804632309779414, mAP tail: 0.4953829750372579
mAP:52.2570080193386
epoch [2/120] batch [10/60] time 0.253 (0.375) data 0.001 (0.068) loss 0.2374 (0.6532) lr 9.9983e-05 eta 0:44:30
epoch [2/120] batch [20/60] time 0.273 (0.355) data 0.000 (0.059) loss 0.1831 (0.3770) lr 9.9983e-05 eta 0:42:04
epoch [2/120] batch [30/60] time 0.188 (0.343) data 0.000 (0.053) loss 0.1784 (0.2495) lr 9.9983e-05 eta 0:40:38
epoch [2/120] batch [40/60] time 0.277 (0.336) data 0.001 (0.048) loss 0.1563 (0.1879) lr 9.9983e-05 eta 0:39:48
epoch [2/120] batch [50/60] time 0.178 (0.332) data 0.001 (0.044) loss 0.1285 (0.1473) lr 9.9983e-05 eta 0:39:14
epoch [2/120] batch [60/60] time 0.147 (0.324) data 0.000 (0.040) loss 0.1179 (0.1327) lr 9.9983e-05 eta 0:38:12
Evaluate on the test set
mAP head: 0.47357801354797585, mAP medium: 0.6804557551751205, mAP tail: 0.4949486870606178
mAP:56.10169950945117
epoch [3/120] batch [10/60] time 0.370 (0.340) data 0.001 (0.056) loss 0.1027 (0.1241) lr 9.9931e-05 eta 0:40:01
epoch [3/120] batch [20/60] time 0.364 (0.332) data 0.000 (0.052) loss 0.1115 (0.1210) lr 9.9931e-05 eta 0:39:05
epoch [3/120] batch [30/60] time 0.299 (0.326) data 0.001 (0.049) loss 0.0993 (0.1159) lr 9.9931e-05 eta 0:38:16
epoch [3/120] batch [40/60] time 0.256 (0.318) data 0.001 (0.046) loss 0.1122 (0.1097) lr 9.9931e-05 eta 0:37:19
epoch [3/120] batch [50/60] time 0.168 (0.313) data 0.001 (0.043) loss 0.1044 (0.1092) lr 9.9931e-05 eta 0:36:38
epoch [3/120] batch [60/60] time 0.226 (0.308) data 0.000 (0.041) loss 0.1254 (0.1073) lr 9.9931e-05 eta 0:36:02
Evaluate on the test set
mAP head: 0.504042944294543, mAP medium: 0.7229871138846619, mAP tail: 0.5075499496732486
mAP:58.803240840366854
epoch [4/120] batch [10/60] time 0.191 (0.320) data 0.003 (0.053) loss 0.1181 (0.1080) lr 9.9846e-05 eta 0:37:21
epoch [4/120] batch [20/60] time 0.362 (0.316) data 0.021 (0.051) loss 0.0938 (0.0996) lr 9.9846e-05 eta 0:36:55
epoch [4/120] batch [30/60] time 0.344 (0.314) data 0.003 (0.048) loss 0.0910 (0.0953) lr 9.9846e-05 eta 0:36:34
epoch [4/120] batch [40/60] time 0.350 (0.309) data 0.001 (0.046) loss 0.0775 (0.0972) lr 9.9846e-05 eta 0:35:58
epoch [4/120] batch [50/60] time 0.210 (0.305) data 0.008 (0.044) loss 0.1002 (0.0973) lr 9.9846e-05 eta 0:35:27
epoch [4/120] batch [60/60] time 0.316 (0.302) data 0.001 (0.042) loss 0.0763 (0.0924) lr 9.9846e-05 eta 0:35:02
Evaluate on the test set
